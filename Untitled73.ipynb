{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "WoNiMdwTb4mP"
      },
      "outputs": [],
      "source": [
        "#Q1. Descrirbe the purpose and  benefits of pooling in CNN.\n",
        "#Ans:-Purpose of Pooling in CNN:\n",
        "#Pooling, specifically Max Pooling and Average Pooling, is a crucial operation in Convolutional Neural Networks (CNNs).\n",
        "#The primary purposes of pooling in CNNs are:\n",
        "#1.Spatial Reduction:Pooling reduces the spatial dimensions of the input feature maps, effectively downsampling them.\n",
        "#This reduction helps manage computational complexity and memory requirements.\n",
        "\n",
        "#02.Translation Invariance:\n",
        "#Pooling introduces a degree of translation invariance.\n",
        "# By summarizing local information (max or average values) in small regions, the network becomes less sensitive to the exact position of features within the receptive field.\n",
        "\n",
        "#03.Feature Retention:While reducing spatial dimensions, pooling retains the most salient features.\n",
        "# This ensures that important information is preserved, facilitating the learning of high-level features in deeper layers of the network.\n",
        "\n",
        "#Benefits of Pooling in CNN:\n",
        "#01.Computational Efficiency:Pooling reduces the amount of computation required in subsequent layers by downsampling the input.\n",
        "# This is especially important in deep networks where the spatial dimensions can become large.\n",
        "\n",
        "#02.Parameter Reduction:Fewer parameters are needed in the network after pooling, leading to a more compact model.\n",
        "# This is beneficial for preventing overfitting, especially when dealing with limited training data.\n",
        "\n",
        "#03.Increased Receptive Field:Pooling allows the network to capture a larger receptive field by summarizing information from a local region.\n",
        "# This enables the network to recognize more global patterns and complex features.\n",
        "\n",
        "#04.Improved Translation Invariance:Pooling enhances the network's ability to detect features regardless of their exact position within the receptive field.\n",
        "# This is crucial for recognizing objects or patterns regardless of their location in an image.\n",
        "\n",
        "#05.Hierarchical Feature Learning:Pooling contributes to the hierarchical learning of features.\n",
        "#As the network progresses through layers, pooling layers help abstract away fine-grained spatial information, focusing on more high-level features.\n",
        "\n",
        "#06.Noise Reduction:Pooling helps filter out noise or irrelevant details from the input, focusing on the most relevant information.\n",
        "#This is beneficial for improving the robustness of the model.\n",
        "\n",
        "#07.Memory Efficiency:The reduced spatial dimensions after pooling lead to lower memory requirements during training and inference, making the model more memory-efficient."
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. Explain the difference between min pooling and max pooling.\n",
        "#Ans:-Min Pooling and Max Pooling in Convolutional Neural Networks (CNNs):\n",
        "#Min pooling and max pooling are two types of pooling operations commonly used in Convolutional Neural Networks (CNNs) for down-sampling feature maps.\n",
        "#Both operations aim to reduce the spatial dimensions of the input while retaining important features.\n",
        "# The key difference lies in how they aggregate information within the pooling regions.\n",
        "#Max Pooling:\n",
        "#Operation:Max pooling involves dividing the input feature map into non-overlapping regions and selecting the maximum value from each region.\n",
        "#Selection Criteria:The maximum value represents the most activated feature in that region.\n",
        "#Benefits:Max pooling is particularly effective in capturing the most prominent features and highlighting the presence of specific patterns within the local regions.\n",
        "#Illustration:If a max pooling operation is applied to a set of values [2, 5, 1, 8], the output will be the maximum value, which is 8.\n",
        "\n",
        "#Min Pooling:\n",
        "#Operation:Min pooling is similar to max pooling but involves selecting the minimum value from each pooling region.\n",
        "#Selection Criteria:The minimum value represents the least activated feature in that region.\n",
        "#Benefits:Min pooling can be useful in scenarios where the goal is to capture the least prominent features or suppress the impact of noise in the input.\n",
        "#Illustration:If a min pooling operation is applied to a set of values [2, 5, 1, 8], the output will be the minimum value, which is 1."
      ],
      "metadata": {
        "id": "2vb-EDsPdyBP"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. Discuss the concept of padding in CNN and its significance.\n",
        "#Ans:-Padding in Convolutional Neural Networks (CNNs):\n",
        "#In the context of CNNs, padding refers to the process of adding extra pixels (or zeros) around the input data before applying convolutional or pooling operations.\n",
        "# Padding is often applied to maintain the spatial dimensions of the input feature maps and to address several issues associated with the convolutional and pooling layers.\n",
        "\n",
        "#Significance of Padding:\n",
        "#Preservation of Spatial Information:Padding helps preserve the spatial information at the edges of the input feature maps.\n",
        "# Without padding, the convolutional operation progressively reduces the spatial dimensions, potentially leading to a loss of information near the borders.\n",
        "\n",
        "#Prevention of Information Loss:Without padding, the convolutional layers tend to lose information from the edges of the input feature maps.\n",
        "# Padding ensures that the outer pixels are given the same consideration as the central pixels during convolution, preventing information loss.\n",
        "\n",
        "#Facilitation of Centered Convolution:Padding enables centered convolution, where the center of the convolutional kernel aligns with the center of the input feature map.\n",
        "# This is important for preserving the spatial relationships between features.\n",
        "\n",
        "#Handling Various Input Sizes:Padding is beneficial when dealing with input images of various sizes.\n",
        "# It allows the network to process inputs of different dimensions consistently and helps in creating a more robust and flexible model.\n",
        "\n",
        "#Mitigation of Border Effects:Without padding, the convolutional operation near the borders can result in fewer interactions with neighboring pixels.\n",
        "# Padding helps mitigate border effects by providing a buffer zone for convolutional operations.\n",
        "\n",
        "#Control over Output Size:Padding allows control over the size of the output feature maps.\n",
        "#By adjusting the amount of padding, one can control the spatial dimensions of the feature maps, influencing the downsampling and upsampling characteristics of the network.\n",
        "\n",
        "#Padding Types:\n",
        "#Valid (No Padding):\n",
        "#Also known as \"no padding\" or \"valid convolution.\"\n",
        "#No padding is added, resulting in reduced spatial dimensions after convolution.\n",
        "#Suitable when the input size is not a concern, and information loss near the borders is acceptable.\n",
        "\n",
        "#Same (Zero Padding):\n",
        "#Padding is added symmetrically to the input so that the output size remains the same.\n",
        "#Commonly used to maintain spatial information and simplify network architectures.\n",
        "\n",
        "#Full Padding:\n",
        "#Padding is added to ensure that every pixel in the input has the same impact on the output.\n",
        "#Less commonly used in practice due to computational requirements."
      ],
      "metadata": {
        "id": "-FrBUqYae62G"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4.Compare and contrast zero-padding and valid-padding in terms of their effects on the output feature map size.\n",
        "#Ans:-Zero-padding (Same Padding) vs. Valid Padding in Convolutional Neural Networks (CNNs):\n",
        "#1. Zero-padding (Same Padding):\n",
        "#Effect on Output Feature Map Size:\n",
        "#Zero-padding involves adding zeros to the input feature map symmetrically, allowing the output feature map to have the same spatial dimensions as the input.\n",
        "#It keeps the spatial information intact and helps mitigate issues related to information loss at the edges.\n",
        "\n",
        "#Advantages:\n",
        "#Preserves spatial information at the borders.\n",
        "#Simplifies network architectures as it keeps the output size consistent.\n",
        "\n",
        "#Use Cases:Commonly used in practice when maintaining spatial information is crucial, such as in image classification tasks.\n",
        "\n",
        "#2. Valid Padding (No Padding):\n",
        "#Effect on Output Feature Map Size:\n",
        "#Valid padding, also known as \"no padding,\" involves not adding any extra padding to the input feature map.\n",
        "#The output feature map size is reduced compared to the input due to the absence of padding.\n",
        "\n",
        "#Advantages:\n",
        "#Reduces computational requirements as no additional operations are performed at the borders.\n",
        "#Suitable when the exact spatial dimensions are not critical, and information loss near the edges is acceptable.\n",
        "\n",
        "#Use Cases:Used when computational efficiency is a priority, and slight information loss at the edges is tolerable.\n",
        "\n",
        "#Comparison:Preservation of Spatial Information:\n",
        "#Zero-padding preserves spatial information, making it suitable for tasks where maintaining accurate spatial relationships is crucial.\n",
        "#Valid padding sacrifices some spatial information at the edges but may be preferred in cases where computational efficiency is prioritized.\n",
        "\n",
        "#Network Architectures:\n",
        "#Zero-padding often simplifies network architectures by keeping the output size consistent.\n",
        "#Valid padding may require additional adjustments in network architecture to account for reduced spatial dimensions.\n",
        "\n",
        "#Use Case Considerations:\n",
        "#Zero-padding is commonly used in tasks like image classification where precise spatial information is essential.\n",
        "#Valid padding is used when computational efficiency is a priority, and a slight reduction in spatial information near the edges is acceptable."
      ],
      "metadata": {
        "id": "qYXShrVagB_q"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Topic : Exploring LeNet"
      ],
      "metadata": {
        "id": "-Ut5BuLCl1lt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1. Provide a brief overview of LeNet-5 architecture.\n",
        "#Ans:-LeNet-5, designed by Yann LeCun and his collaborators in 1998, is one of the pioneering convolutional neural network (CNN) architectures.\n",
        "# It was primarily developed for handwritten digit recognition tasks, specifically for recognizing digits in postal codes and checks.\n",
        "# LeNet-5 played a crucial role in demonstrating the effectiveness of deep learning in computer vision.\n",
        "#Key Components of LeNet-5:\n",
        "#01.Input Layer:LeNet-5 takes as input grayscale images of size 32x32 pixels.\n",
        "\n",
        "#02.First Convolutional Layer (C1):\n",
        "#Convolutional layer with a kernel size of 5x5.\n",
        "#Output channels: 6.\n",
        "#Activation function: Sigmoid.\n",
        "#Subsampling (Pooling): Average pooling with a 2x2 kernel and a stride of 2.\n",
        "\n",
        "#03.Second Convolutional Layer (C3):\n",
        "#Convolutional layer with a kernel size of 5x5.\n",
        "#Input channels: 6.\n",
        "#Output channels: 16.\n",
        "#Activation function: Sigmoid.\n",
        "#Subsampling (Pooling): Average pooling with a 2x2 kernel and a stride of 2.\n",
        "\n",
        "#04.Third Convolutional Layer (C5):\n",
        "#Convolutional layer with a kernel size of 5x5.\n",
        "#Input channels: 16.\n",
        "#Output channels: 120.\n",
        "#Activation function: Sigmoid.\n",
        "\n",
        "#05.Fully Connected Layers (F6 and Output Layer):\n",
        "#F6: Fully connected layer with 84 neurons.\n",
        "#Output layer: Fully connected layer with 10 neurons (for 10 output classes in digit recognition).\n",
        "#Activation function: Sigmoid for F6, Softmax for the output layer.\n",
        "\n",
        "#06.Flatten Layers:Flatten layers are used to convert the 3D feature maps into a 1D vector before the fully connected layers.\n",
        "\n",
        "#07.Activation Function:Sigmoid activation functions are used throughout the network except for the output layer, which uses the Softmax activation for multi-class classification.\n",
        "\n",
        "#08.Loss Function:Cross-entropy loss is typically used as the loss function for the classification task.\n",
        "\n",
        "#Advantages and Contributions:\n",
        "#First Convolutional Neural Network:\n",
        "#LeNet-5 was one of the earliest CNN architectures and demonstrated the effectiveness of convolutional layers in feature extraction and spatial hierarchies.\n",
        "#Parameter Sharing:Parameter sharing in convolutional layers reduces the number of parameters, making the model more efficient.\n",
        "#Pooling Layers:LeNet-5 introduced the concept of pooling layers for spatial down-sampling, which aids in creating hierarchical and translation-invariant features.\n",
        "#Demonstrated Success in Handwriting Recognition:LeNet-5 achieved high accuracy in handwritten digit recognition tasks and laid the foundation for subsequent advancements in CNNs for image recognition."
      ],
      "metadata": {
        "id": "qiqOfW0Gl8Ld"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. Describe the key components of LeNet-5 and their respective purposes.\n",
        "#Ans:-Key Components of LeNet-5 and Their Purposes:\n",
        "#01.Input Layer:\n",
        "#Purpose: Accepts grayscale images as input with dimensions 32x32 pixels.\n",
        "#Details: The input layer represents the raw pixel values of the input images.\n",
        "\n",
        "#First Convolutional Layer (C1):\n",
        "#Purpose: Performs the initial feature extraction.\n",
        "#Details:\n",
        "#Convolution with a 5x5 kernel.\n",
        "#Input channels: 1 (grayscale).\n",
        "#Output channels: 6.\n",
        "#Activation function: Sigmoid.\n",
        "#Subsampling (Pooling): Average pooling with a 2x2 kernel and a stride of 2.\n",
        "#Significance: C1 extracts basic features from the input images.\n",
        "\n",
        "#Second Convolutional Layer (C3):\n",
        "#Purpose: Further refines features and extracts higher-level representations.\n",
        "#Details:\n",
        "#Convolution with a 5x5 kernel.\n",
        "#Input channels: 6.\n",
        "#Output channels: 16.\n",
        "#Activation function: Sigmoid.\n",
        "#Subsampling (Pooling): Average pooling with a 2x2 kernel and a stride of 2.\n",
        "#Significance: C3 builds upon the features extracted by C1, creating more complex representations.\n",
        "\n",
        "#Third Convolutional Layer (C5):\n",
        "#Purpose: Continues feature extraction and prepares for fully connected layers.\n",
        "#Details:\n",
        "#Convolution with a 5x5 kernel.\n",
        "#Input channels: 16.\n",
        "#Output channels: 120.\n",
        "#Activation function: Sigmoid.\n",
        "#Significance: C5 further abstracts features and prepares for the transition to fully connected layers.\n",
        "\n",
        "#Fully Connected Layers (F6 and Output Layer):\n",
        "#Purpose: Make predictions based on the learned features.\n",
        "#Details:\n",
        "#F6: Fully connected layer with 84 neurons and a Sigmoid activation function.\n",
        "#Output layer: Fully connected layer with 10 neurons (for digit classes) and a Softmax activation function.\n",
        "#Significance: These layers combine the abstracted features for classification.\n",
        "\n",
        "#Flatten Layers:\n",
        "#Purpose: Reshape the 3D feature maps into 1D vectors.\n",
        "#Details: Flatten layers precede the fully connected layers."
      ],
      "metadata": {
        "id": "r3fdswDFuBtK"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. Discuss the advantages and limitations of LeNet-5 and their context of image classification tasks.\n",
        "#Ans:Advantages of LeNet-5 in Image Classification:\n",
        "#01.Pioneering CNN Architecture:LeNet-5 was one of the pioneering architectures that demonstrated the effectiveness of convolutional neural networks (CNNs) for image classification tasks.\n",
        "# It laid the foundation for subsequent developments in deep learning.\n",
        "\n",
        "#02.Feature Hierarchy:\n",
        "#LeNet-5 introduced the concept of a feature hierarchy through convolutional and pooling layers.\n",
        "# This hierarchical representation allows the network to progressively extract complex features.\n",
        "\n",
        "#03.Parameter Sharing:Parameter sharing in convolutional layers reduces the number of parameters, making the model more efficient and reducing the risk of overfitting, especially in the presence of limited training data.\n",
        "\n",
        "#04.Translation Invariance:The use of pooling layers in LeNet-5 contributes to translation invariance, enabling the network to recognize features regardless of their exact position in the input.\n",
        "\n",
        "#05.Demonstrated Success in Handwriting Recognition:LeNet-5 achieved high accuracy in handwritten digit recognition tasks, showcasing its effectiveness in recognizing patterns and shapes.\n",
        "\n",
        "#06.Sigmoid Activation:The use of the sigmoid activation function in LeNet-5's layers allows the model to learn non-linear mappings, enhancing its capability to capture complex relationships in the data.\n",
        "\n",
        "#Limitations of LeNet-5 in Image Classification:\n",
        "#01.Sigmoid Activation:The use of the sigmoid activation function in the layers can lead to the vanishing gradient problem,\n",
        "# making it challenging for the model to learn and update parameters effectively, especially in deeper networks.\n",
        "\n",
        "#02.Limited Capacity:Compared to modern architectures, LeNet-5 has a relatively limited capacity to capture complex patterns and variations in large datasets.\n",
        "# Deeper and more complex architectures have since been developed to handle the challenges of diverse image datasets.\n",
        "\n",
        "#03.Small Input Size:LeNet-5 was designed for small input images (32x32 pixels).\n",
        "# While suitable for certain tasks, it may struggle with more detailed or high-resolution images commonly encountered in modern computer vision applications.\n",
        "\n",
        "#04.Sigmoid Saturation:Sigmoid activation functions can saturate, leading to issues like the vanishing gradient problem, where gradients become very small during backpropagation, hindering effective weight updates.\n",
        "\n",
        "#05.Limited Non-Linearity:The overall architecture of LeNet-5 may exhibit limited non-linearity compared to more recent architectures, potentially restricting its ability to model highly complex relationships in data.\n",
        "\n",
        "#06.Sensitivity to Initialization:LeNet-5 and similar architectures can be sensitive to weight initialization, and the effectiveness of training may depend on the chosen initialization strategy.\n",
        "\n",
        "#07.Not Suited for Large and Diverse Datasets:LeNet-5 may struggle to handle the diversity and complexity present in large datasets, limiting its applicability to more challenging image classification tasks."
      ],
      "metadata": {
        "id": "k9foKoJ_yIr4"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4. Implement LeNet-5 using a deep learning framework of your choice (e.g., TensorFlow, PyTorch) and train it on a publicly available dataset (e.g., MNIST). Evaluate its performance and provide insights.\n",
        "#Ans:"
      ],
      "metadata": {
        "id": "K2YSGIb_0DzK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KI0lLbFI0_3_",
        "outputId": "87751067-e057-4da6-9a97-0c4321ebeb9a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "\n",
        "# LeNet-5 Architecture\n",
        "class LeNet5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(LeNet5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 6, kernel_size=5, stride=1, padding=2)\n",
        "        self.act1 = nn.Sigmoid()\n",
        "        self.pool1 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv2 = nn.Conv2d(6, 16, kernel_size=5, stride=1)\n",
        "        self.act2 = nn.Sigmoid()\n",
        "        self.pool2 = nn.AvgPool2d(kernel_size=2, stride=2)\n",
        "\n",
        "        self.conv3 = nn.Conv2d(16, 120, kernel_size=5, stride=1)\n",
        "        self.act3 = nn.Sigmoid()\n",
        "\n",
        "        self.flatten = nn.Flatten()\n",
        "\n",
        "        self.fc1 = nn.Linear(120, 84)\n",
        "        self.act4 = nn.Sigmoid()\n",
        "\n",
        "        self.fc2 = nn.Linear(84, 10)\n",
        "        self.act5 = nn.Softmax(dim=1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.pool1(self.act1(self.conv1(x)))\n",
        "        x = self.pool2(self.act2(self.conv2(x)))\n",
        "        x = self.act3(self.conv3(x))\n",
        "        x = self.flatten(x)\n",
        "        x = self.act4(self.fc1(x))\n",
        "        x = self.act5(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "# Load MNIST dataset\n",
        "transform = transforms.Compose([transforms.ToTensor(), transforms.Normalize((0.5,), (0.5,))])\n",
        "train_dataset = datasets.MNIST(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.MNIST(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "# Set up data loaders\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize the model, optimizer, and loss function\n",
        "model = LeNet5()\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 10\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluation on the test set\n",
        "model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        outputs = model(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K9Q4X2PF1QWS",
        "outputId": "170cd644-7d8c-4fd4-ce86-586dc0a95a62"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 138084457.57it/s]"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 86897915.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 40213580.71it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 14688148.63it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Epoch 1/10, Loss: 1.9387245119761811\n",
            "Epoch 2/10, Loss: 1.6223456747750484\n",
            "Epoch 3/10, Loss: 1.5340590540534143\n",
            "Epoch 4/10, Loss: 1.5115797218483393\n",
            "Epoch 5/10, Loss: 1.501482479607881\n",
            "Epoch 6/10, Loss: 1.4952360626731092\n",
            "Epoch 7/10, Loss: 1.4908081546012781\n",
            "Epoch 8/10, Loss: 1.4883036397413405\n",
            "Epoch 9/10, Loss: 1.4865467439073998\n",
            "Epoch 10/10, Loss: 1.4840894350365026\n",
            "Test Accuracy: 98.06%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TOPIC: Analyzing AlexNet"
      ],
      "metadata": {
        "id": "TudFJ4LT3JKz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Q1. Provide an overview of the AlexNet architecture.\n",
        "#Ans:-AlexNet Architecture Overview:\n",
        "#AlexNet, introduced by Alex Krizhevsky, Ilya Sutskever, and Geoffrey Hinton in 2012, was a groundbreaking deep convolutional neural network (CNN) that significantly advanced the field of computer vision.\n",
        "# AlexNet was designed to participate in the ImageNet Large Scale Visual Recognition Challenge (ILSVRC) and achieved a substantial improvement in image classification accuracy compared to previous methods.\n",
        "# Here's an overview of the key components of the AlexNet architecture:\n",
        "#01.Input Layer:\n",
        "#Accepts RGB images with a fixed size of 224x224 pixels.\n",
        "\n",
        "#02.Convolutional Layers (Conv1 to Conv5):\n",
        "#Conv1:\n",
        "#96 filters with a kernel size of 11x11.\n",
        "#Stride of 4 pixels.\n",
        "#Rectified Linear Unit (ReLU) activation.\n",
        "#Local Response Normalization (LRN).\n",
        "\n",
        "#Conv2:\n",
        "#256 filters with a kernel size of 5x5.\n",
        "#Stride of 1 pixel.\n",
        "#ReLU activation.\n",
        "#LRN.\n",
        "#Max-pooling with a 3x3 kernel and a stride of 2 pixels.\n",
        "\n",
        "#Conv3:\n",
        "#384 filters with a kernel size of 3x3.\n",
        "#Stride of 1 pixel.\n",
        "#ReLU activation.\n",
        "\n",
        "#Conv4:\n",
        "#384 filters with a kernel size of 3x3.\n",
        "#Stride of 1 pixel.\n",
        "#ReLU activation.\n",
        "\n",
        "#Conv5:\n",
        "#256 filters with a kernel size of 3x3.\n",
        "#Stride of 1 pixel.\n",
        "#ReLU activation.\n",
        "#Max-pooling with a 3x3 kernel and a stride of 2 pixels.\n",
        "\n",
        "#03.Fully Connected Layers (FC6 to FC8):\n",
        "#FC6:\n",
        "#4096 neurons.\n",
        "#ReLU activation.\n",
        "#Dropout for regularization.\n",
        "\n",
        "#FC7:\n",
        "#4096 neurons.\n",
        "#ReLU activation.\n",
        "#Dropout.\n",
        "\n",
        "#FC8:\n",
        "#1000 neurons (output classes for ILSVRC).\n",
        "#Softmax activation for classification.\n",
        "\n",
        "#04.Normalization and Pooling:\n",
        "#Local Response Normalization (LRN) is applied in Conv1 and Conv2 to enhance generalization.\n",
        "#Max-pooling is used in Conv1, Conv2, and Conv5 layers.\n",
        "\n",
        "#05.Activation Function:\n",
        "#Rectified Linear Unit (ReLU) is used as the activation function throughout the convolutional and fully connected layers, except for the output layer where Softmax is applied for classification.\n",
        "\n",
        "#06.Dropout:\n",
        "#Dropout is used in FC6 and FC7 layers during training for regularization, preventing overfitting.\n",
        "\n",
        "#07.Output Layer:\n",
        "#The output layer has 1000 neurons corresponding to the 1000 classes in the ImageNet dataset.\n",
        "#Softmax activation is applied for multi-class classification.\n",
        "\n",
        "#Contributions and Impact:\n",
        "#AlexNet significantly advanced the field of deep learning and played a crucial role in popularizing deep CNNs for image classification tasks.\n",
        "#It demonstrated the effectiveness of deep neural networks, particularly CNNs, in handling complex visual data.\n",
        "#The use of ReLU activation, local response normalization, and dropout contributed to improved training convergence and generalization."
      ],
      "metadata": {
        "id": "PDvGuAdr3LHm"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q2. Explain the architertural innovations introduced in AlexNet that contriruted to its breakthrough performance.\n",
        "#Ans:-AlexNet introduced several architectural innovations that contributed to its breakthrough performance in image classification tasks.\n",
        "#These innovations addressed challenges in training deep neural networks and significantly improved the model's ability to learn and generalize.\n",
        "# Here are the key architectural innovations in AlexNet:\n",
        "#01.Deep Architecture:\n",
        "#AlexNet was one of the first deep convolutional neural networks (CNNs) with a significantly deep architecture, consisting of eight layers, including five convolutional layers and three fully connected layers.\n",
        "# This depth allowed the network to learn hierarchical features of increasing complexity.\n",
        "\n",
        "#02.Rectified Linear Units (ReLU):AlexNet replaced traditional activation functions like sigmoid or hyperbolic tangent with Rectified Linear Units (ReLU) in the hidden layers.\n",
        "# ReLU introduces non-linearity by activating neurons with a simple threshold function (outputting the input for positive values and zero for negative values).\n",
        "#ReLU helps mitigate the vanishing gradient problem and accelerates convergence during training.\n",
        "\n",
        "#03.Local Response Normalization (LRN):LRN was applied after the first and second convolutional layers (Conv1 and Conv2).\n",
        "#LRN normalizes the responses within a local neighborhood, enhancing the contrast between activated and non-activated neurons.\n",
        "# This kind of normalization acts as a form of lateral inhibition and helps improve the generalization of the model.\n",
        "\n",
        "#04.Overlapping Pooling:AlexNet introduced overlapping max-pooling operations, which allowed pooling regions to overlap, unlike traditional non-overlapping pooling.\n",
        "# Overlapping pooling helps capture more spatial information and improves the model's robustness to spatial translations.\n",
        "\n",
        "#05.Data Augmentation and Dropout:\n",
        "#To address overfitting, AlexNet employed two regularization techniques: data augmentation and dropout.\n",
        "#Data Augmentation: The training set was artificially expanded by applying random transformations (e.g., rotations, flips, and crops) to the input images.\n",
        "# This helped the model generalize better to variations in the input data.\n",
        "#Dropout: Dropout was applied to the fully connected layers (FC6 and FC7) during training.\n",
        "# Dropout randomly drops a fraction of neurons during each forward and backward pass, preventing co-adaptation of neurons and improving model generalization."
      ],
      "metadata": {
        "id": "4Erpy6YY5mp3"
      },
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q3. Discuss the role of convolutional layers, pooling layers, and fully connected layers in AlexNet.\n",
        "#Ans:-Role of Convolutional Layers, Pooling Layers, and Fully Connected Layers in AlexNet:\n",
        "#01.Convolutional Layers:\n",
        "#Role:\n",
        "#Convolutional layers are fundamental in capturing local patterns and spatial hierarchies within the input image. They learn to detect features such as edges, textures, and simple shapes.\n",
        "#Implementation in AlexNet:\n",
        "#AlexNet includes five convolutional layers (Conv1 to Conv5) that use filters of varying sizes (e.g., 11x11, 5x5, and 3x3) to capture features at different scales.\n",
        "#ReLU activation functions are applied after each convolutional operation, introducing non-linearity.\n",
        "\n",
        "#02.Pooling Layers:\n",
        "#Role:\n",
        "#Pooling layers down-sample the spatial dimensions of the feature maps, reducing computational complexity and making the model more robust to variations in object position and scale.\n",
        "#Implementation in AlexNet:\n",
        "#AlexNet employs max-pooling in Conv1, Conv2, and Conv5. Max-pooling retains the most prominent features within each pooling region.\n",
        "#Overlapping pooling (pooling regions with overlap) is used to capture more spatial information.\n",
        "\n",
        "#03.Fully Connected Layers:\n",
        "#Role:\n",
        "#Fully connected layers at the end of the network are responsible for high-level reasoning and decision-making based on the hierarchical features learned by the convolutional layers.\n",
        "#They combine local features from convolutional layers across the entire input space.\n",
        "\n",
        "#04.Implementation in AlexNet:\n",
        "#AlexNet has three fully connected layers (FC6 to FC8) at the end of the architecture.\n",
        "#FC6 and FC7 have 4096 neurons each with ReLU activation and dropout for regularization.\n",
        "#FC8 is the output layer with 1000 neurons (representing the classes in ImageNet) and softmax activation for classification.\n",
        "\n",
        "#05.Role of Data Augmentation and Dropout:\n",
        "#Data Augmentation:\n",
        "#Data augmentation, although not a specific layer, is a crucial part of the training process in AlexNet.\n",
        "# It involves applying random transformations (rotations, flips, crops) to the input images during training, effectively expanding the dataset and improving generalization.\n",
        "\n",
        "#Dropout:\n",
        "#Dropout is applied to the fully connected layers (FC6 and FC7) during training.\n",
        "#It randomly drops a fraction of neurons during each forward and backward pass, preventing overfitting and encouraging the network to be more robust.\n",
        "\n",
        "#05.Overall Flow:\n",
        "#The convolutional layers capture low to mid-level features in the input images.\n",
        "#The pooling layers down-sample the spatial dimensions, preserving essential information.\n",
        "#Fully connected layers combine the high-level features and make final predictions.\n",
        "\n",
        "#06.Softmax Activation in Output Layer:\n",
        "#The softmax activation in the output layer (FC8) converts the raw scores into class probabilities, facilitating multi-class classification."
      ],
      "metadata": {
        "id": "GY8pNXak7dcp"
      },
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Q4.Implement AlexNet using a deep learning framewock of your choice and evaluate its perfocmance on a dataset of your choice."
      ],
      "metadata": {
        "id": "-atszVZu_MZM"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install torch torchvision"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "legyeQc1enRT",
        "outputId": "a98d36bb-edba-4f46-d513-d0ec70b78f72"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (2.1.0+cu118)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.10/dist-packages (0.16.0+cu118)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch) (3.13.1)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch) (1.12)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch) (3.2.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch) (3.1.2)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch) (2023.6.0)\n",
            "Requirement already satisfied: triton==2.1.0 in /usr/local/lib/python3.10/dist-packages (from torch) (2.1.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from torchvision) (1.23.5)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision) (2.31.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.10/dist-packages (from torchvision) (9.4.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch) (2.1.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision) (2023.11.17)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch) (1.3.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.models as models\n",
        "\n",
        "# Load CIFAR-10 dataset\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "train_dataset = datasets.CIFAR10(root=\"./data\", train=True, transform=transform, download=True)\n",
        "test_dataset = datasets.CIFAR10(root=\"./data\", train=False, transform=transform, download=True)\n",
        "\n",
        "train_loader = DataLoader(dataset=train_dataset, batch_size=64, shuffle=True)\n",
        "test_loader = DataLoader(dataset=test_dataset, batch_size=64, shuffle=False)\n",
        "\n",
        "# Initialize AlexNet model\n",
        "alexnet = models.alexnet()\n",
        "\n",
        "# Modify the last fully connected layer for CIFAR-10\n",
        "num_classes = 10\n",
        "alexnet.classifier[6] = nn.Linear(4096, num_classes)\n",
        "\n",
        "# Set the device\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "alexnet = alexnet.to(device)\n",
        "\n",
        "# Define loss function and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)\n",
        "\n",
        "# Training loop\n",
        "num_epochs = 5\n",
        "for epoch in range(num_epochs):\n",
        "    alexnet.train()\n",
        "    running_loss = 0.0\n",
        "    for images, labels in train_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = alexnet(images)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    print(f\"Epoch {epoch + 1}/{num_epochs}, Loss: {running_loss / len(train_loader)}\")\n",
        "\n",
        "# Evaluation on the test set\n",
        "alexnet.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, labels in test_loader:\n",
        "        images, labels = images.to(device), labels.to(device)\n",
        "\n",
        "        outputs = alexnet(images)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "accuracy = correct / total\n",
        "print(f\"Test Accuracy: {accuracy * 100:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pmX_0rcheoXP",
        "outputId": "6da6aadd-dfc9-454f-a86b-de3d3bd91e8b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz to ./data/cifar-10-python.tar.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 170498071/170498071 [00:01<00:00, 90660265.64it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/cifar-10-python.tar.gz to ./data\n",
            "Files already downloaded and verified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "eeDL0XONexip"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}